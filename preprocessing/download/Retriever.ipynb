{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrI81Xn6wh0d"
   },
   "source": [
    "# Tweets Retriver\n",
    "\n",
    "Tweets IDs: [COVID19_Tweets_dataset GitHub repository](https://github.com/lopezbec/COVID19_Tweets_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from IPython.display import clear_output\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install twarc\n",
    "# !pip install jsonlines\n",
    "# !pip install wget\n",
    "# !pip install python-dotenv\n",
    "# !pip install tqdm\n",
    "\n",
    "# # clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "et0_5DEEFNpW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incomplete credentials provided.\n",
      "Please run the command \"twarc configure\" to get started.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import jsonlines, json, csv\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import tqdm\n",
    "from twarc import Twarc\n",
    "import glob\n",
    "\n",
    "# These keys are received after applying for a twitter developer account\n",
    "import jsonlines, json, csv\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'202111': {'start_date': '2021-11-01', 'end_date': '2021-11-30'},\n",
       " '202112': {'start_date': '2021-12-01', 'end_date': '2021-12-31'},\n",
       " '202201': {'start_date': '2022-1-01', 'end_date': '2022-1-31'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Paths\n",
    "data_url = \"https://raw.githubusercontent.com/lopezbec/COVID19_Tweets_Dataset_2021/master/Summary_Details/\"\n",
    "hydrated_dir = \"Hdrated_tweets/\"\n",
    "tweet_ID_dir = \"Tweet_IDs/\"\n",
    "tweet_summary_dir = \"Tweet_Summary/\"\n",
    "# create a folder to store tweet IDs if not exists\n",
    "os.makedirs(tweet_summary_dir, exist_ok=True)\n",
    "os.makedirs(tweet_ID_dir, exist_ok=True)\n",
    "os.makedirs(tweet_ID_dir, exist_ok=True)\n",
    "\n",
    "## define months to study\n",
    "data_month_dict = {\n",
    "    \"202111\": {\n",
    "        \"start_date\": \"2021-11-01\",\n",
    "        \"end_date\": \"2021-11-30\"},\n",
    "    \"202112\": {\n",
    "        \"start_date\": \"2021-12-01\",\n",
    "        \"end_date\": \"2021-12-31\"},    \n",
    "    \"202201\": {\n",
    "        \"start_date\": \"2022-1-01\",\n",
    "        \"end_date\": \"2022-1-31\"},    \n",
    "}\n",
    "\n",
    "data_hours = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COVID-19 Tweet ids from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_month, date_range in data_month_dict.items():\n",
    "    start_date = date_range[\"start_date\"]\n",
    "    end_date = date_range[\"end_date\"]\n",
    "    \n",
    "    dates_list = pd.date_range(start_date, end_date).tolist()\n",
    "    month_str = dates_list[0].strftime(\"%Y_%m\")\n",
    "    dates_list = [d.strftime(\"%Y_%m_%d\") for d in dates_list]\n",
    "\n",
    "    files_list = [\n",
    "        f\"{data_url}{month_str}/{date_str}_{hour_str}_Summary_Details.csv\"\n",
    "        for date_str, hour_str\n",
    "        in itertools.product(dates_list, data_hours)\n",
    "    ]\n",
    "    \n",
    "    month_directory = f\"{tweet_summary_dir}{data_month}\"\n",
    "    os.makedirs(month_directory, exist_ok=True)\n",
    "    for file in files_list:\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if not os.path.exists(filename):\n",
    "            try:\n",
    "                wget.download(file, out=month_directory)\n",
    "            except:\n",
    "                print(\"someting goes wrong\")\n",
    "                # there are some known gaps with no data collected:\n",
    "                # https://github.com/lopezbec/COVID19_Tweets_Dataset#data-collection-process-inconsistencies\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "uLREJkIRjvDh",
    "outputId": "ffe0663d-6df3-431e-bf19-017dc7b73ede"
   },
   "outputs": [],
   "source": [
    "# create a folder to store tweet IDs if not exists\n",
    "os.makedirs(tweet_ID_dir, exist_ok=True)\n",
    "\n",
    "for data_month, date_range in data_month_dict.items():\n",
    "    print(data_month)\n",
    "    files = glob.glob(f\"{tweet_summary_dir}{data_month}\")\n",
    "    tweet_ids = []\n",
    "    for file in tqdm.tqdm(files):\n",
    "        data = pd.read_csv(files)\n",
    "\n",
    "        # only keep English tweets\n",
    "        data = data[data['Language']=='en']\n",
    "        # filter out retweets\n",
    "        data = data[data[\"RT\"]==\"NO\"] \n",
    "        tweet_ids.extend(data[\"Tweet_ID\"])\n",
    "\n",
    "    # write Tweet IDs to a file for hydration later\n",
    "    tweet_ids_filename = f\"{tweet_ID_dir}/{data_month}.txt\"\n",
    "    with open(tweet_ids_filename, \"w+\") as f:\n",
    "        for tweet_id in tweet_ids:\n",
    "            f.write(f\"{tweet_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15067558 Tweet_IDs/11.txt\n",
      "23427560 Tweet_IDs/12.txt\n"
     ]
    }
   ],
   "source": [
    "# check total tweet IDs for each month\n",
    "for data_month in data_month_dict.keys():\n",
    "    !wc -l \"Tweet_IDs/{data_month}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 10  ## split the data to make the files smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through tweet IDs for each month and sample 10%\n",
    "for data_month in data_month_dict.keys():\n",
    "    filename = f\"{tweet_ID_dir}{data_month}.txt\"\n",
    "    print(filename)\n",
    "    # read monthly tweet IDs\n",
    "    tweet_ids = pd.read_csv(filename, header=None, dtype=str)\n",
    "    \n",
    "    # split the data frame into n chunks\n",
    "    end_i = tweet_ids.shape[0]\n",
    "    chunk_size = math.ceil(end_i / n_split)\n",
    "\n",
    "    # iterate through all the chunks and output to file\n",
    "    for i, start_i in enumerate(range(0, end_i, chunk_size)):\n",
    "        tweet_split_i = tweet_ids[start_i:start_i + chunk_size]\n",
    "\n",
    "        # output to a file for each split\n",
    "        tweet_sample_ids_filename = f\"{tweet_ID_dir}{data_month}_{i}.txt\"\n",
    "        with open(tweet_sample_ids_filename, \"w+\") as f:\n",
    "            for tweet_id in tweet_split_i[0]:\n",
    "                f.write(f\"{tweet_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total tweet IDs for each month\n",
    "for data_month in data_month_dict.keys():\n",
    "    for i in range(n_split):\n",
    "        !wc -l \"Tweet_IDs/{data_month}_{i}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxFa0jOTKbBw"
   },
   "source": [
    "# Hydrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBBH-a4WK1JM"
   },
   "source": [
    "### Set up output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "both",
    "id": "9ATxyEfSLBK1"
   },
   "outputs": [],
   "source": [
    "output_dir= \"../Dropbox/\"\n",
    "os.makedirs(out,output_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through tweet IDs for each month and sample 10%\n",
    "for data_month in data_month_dict.keys():\n",
    "    for i in range(0, n_split):\n",
    "        tweet_ids_filename = f\"Tweet_IDs/{data_month}_{i}.txt\" #@param {type: \"string\"}\n",
    "        output_filename = f\"{output_dir}{data_month}/{data_month}_{i}.txt\" #@param {type: \"string\"}\n",
    "        print(\"On file %s\"%output_filename)\n",
    "        ids = []\n",
    "        with open(tweet_ids_filename, \"r\") as ids_file:\n",
    "            ids = ids_file.read().split()\n",
    "        hydrated_tweets = []\n",
    "        ids_to_hydrate = set(ids)\n",
    "        # Check hydrated tweets\n",
    "        if os.path.isfile(output_filename):\n",
    "            with jsonlines.open(output_filename, \"r\") as reader:\n",
    "                for i in reader.iter(type=dict, skip_invalid=True):\n",
    "                    hydrated_tweets.append(i)\n",
    "                    ids_to_hydrate.remove(i[\"id_str\"])\n",
    "        if ids_to_hydrate == 0:\n",
    "            print(\"Finished downloading. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
    "        print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
    "        \n",
    "        pbar = tqdm(total=len(ids_to_hydrate))\n",
    "        count = len(hydrated_tweets)\n",
    "        start_index = count\n",
    "\n",
    "        num_save  = 10000\n",
    "\n",
    "        # start hydrating\n",
    "        for tweet in t.hydrate(ids_to_hydrate):\n",
    "            hydrated_tweets.append(tweet)\n",
    "            count += 1\n",
    "            # If num_save iterations have passed,\n",
    "            if (count % num_save) == 0:\n",
    "                with jsonlines.open(output_filename, \"a\") as writer:\n",
    "                    for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "                        writer.write(hydrated_tweet)\n",
    "                start_index = count\n",
    "            pbar.update(1)\n",
    "\n",
    "        if count != start_index:\n",
    "            print(\"Here with start_index\", start_index)\n",
    "            with jsonlines.open(output_filename, \"a\") as writer:\n",
    "                for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "                    writer.write(hydrated_tweet)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG9cS-aoW0Wy"
   },
   "source": [
    "#### This takes a long time, several days for a month. You can convert this to a python file. Jupyter notebook disconnects sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utvrm8g7NHNs"
   },
   "source": [
    "## Convert jsonl files that are stored in .txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "data_path = \"../Dropbox/\"\n",
    "\n",
    "folders = sorted([f for f in glob.glob(data_path+\"*\") if \"-\" in f])\n",
    "## convert jsonl to csv files\n",
    "files = []\n",
    "for folder in folders:\n",
    "     files += glob.glob(folder+\"/*\") \n",
    "files = sorted(files)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/212 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file ../../Dropbox/COVIDTweets/2021-09/9_4.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 212/212 [07:29<00:00,  2.12s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Convert jsonl to csv\n",
    "for file in tqdm(files):\n",
    "    print(\"On file %s\"%(file))\n",
    "    output_filename = file.replace(\"txt\",\"csv\") \n",
    "    # These are the column name that are selected to be stored in the csv\n",
    "    keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
    "      \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
    "      \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
    "      \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
    "      \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
    "      \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
    "      \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
    "      \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
    "      \"quoted_status_permalink\"]\n",
    "    hydrated_tweets = []\n",
    "    # Reads the current tweets\n",
    "    with jsonlines.open(file, \"r\") as reader:\n",
    "        for hydrated in reader.iter(type=dict, skip_invalid=True):\n",
    "            hydrated_tweets.append(hydrated)\n",
    "    # Writes them out\n",
    "    with open(output_filename, \"w+\") as output_file:\n",
    "        d = csv.DictWriter(output_file, keyset)\n",
    "        d.writeheader()\n",
    "        d.writerows(hydrated_tweets)\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RDzd7FUKFviv"
   ],
   "name": "Automatically_Hydrate_TweetsIDs_COVID19_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
