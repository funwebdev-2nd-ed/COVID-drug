{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran NER using [Twitter-Stanza](https://github.com/social-machines/TweebankNLP). Follow their README for more detailed introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import glob\n",
    "from pandarallel import pandarallel \n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:01:36 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../Twee...kenizer.pt |\n",
      "| pos       | ../../Twee..._tagger.pt |\n",
      "| lemma     | ../../Twee...matizer.pt |\n",
      "| depparse  | ../../Twee..._parser.pt |\n",
      "| ner       | ../../Twee...rtagger.pt |\n",
      "=======================================\n",
      "\n",
      "2022-04-06 12:01:36 INFO: Use device: gpu\n",
      "2022-04-06 12:01:36 INFO: Loading: tokenize\n",
      "2022-04-06 12:01:36 INFO: Loading: pos\n",
      "2022-04-06 12:01:43 INFO: Loading: lemma\n",
      "2022-04-06 12:01:43 INFO: Loading: depparse\n",
      "2022-04-06 12:01:44 INFO: Loading: ner\n",
      "2022-04-06 12:01:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# config for the `en_tweet` models (models trained only on Tweebank)\n",
    "config = {\n",
    "          'processors': 'tokenize,lemma,pos,depparse,ner',\n",
    "          'lang': 'en',\n",
    "          'tokenize_pretokenized': True, # disable tokenization\n",
    "          'tokenize_model_path': '../../TweebankNLP/twitter-stanza/saved_models/tokenize/en_tweet_tokenizer.pt',\n",
    "          'lemma_model_path': '../../TweebankNLP/twitter-stanza/saved_models/lemma/en_tweet_lemmatizer.pt',\n",
    "          \"pos_model_path\": '../../TweebankNLP/twitter-stanza/saved_models/pos/en_tweetewt_tagger.pt',\n",
    "          \"depparse_model_path\": '../../TweebankNLP/twitter-stanza/saved_models/depparse/en_tweetewt_parser.pt',\n",
    "          \"ner_model_path\": '../../TweebankNLP/twitter-stanza/saved_models/ner/en_tweetwnut17_nertagger.pt',\n",
    "}\n",
    "\n",
    "# Initialize the pipeline using a configuration dict\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(**config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NER(x):\n",
    "    ners = []\n",
    "    types = []\n",
    "\n",
    "    doc = nlp(x)\n",
    "    for sent in doc.sentences:\n",
    "        if sent.ents:\n",
    "            for ent in sent.ents:\n",
    "                ners.append(ent.text)\n",
    "                types.append(ent.type)\n",
    "    return \"|\".join(ners) + \"%\" + \"|\".join(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/final/\"\n",
    "files = sorted(glob.glob(f\"{data_dir}*.csv\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "data_dir = \"../data/ner/\"\n",
    "os.makedirs(data_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/final/remdesivir.csv',\n",
       " '../data/final/molnupiravir.csv',\n",
       " '../data/final/ivermectin.csv',\n",
       " '../data/final/hcq.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252198/252198 [7:13:38<00:00,  9.69it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "for file in files[3:]:\n",
    "    drug = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(drug)\n",
    "    df = pd.read_csv(file, lineterminator=\"\\n\", low_memory=False)\n",
    "    df = df[df.stance!=0]\n",
    "    df[\"ner\"] = df.full_text.progress_apply(lambda x: find_NER(x))\n",
    "    df = df[[\"stance\",\"wave\",\"ner\"]]\n",
    "    df[[\"ner\",\"type\"]] = df.ner.str.split('%', expand=True)\n",
    "    df.to_csv(f\"{data_dir}{drug}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import random\n",
    "import string\n",
    "import glob\n",
    "import tqdm\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(0, 50)\n",
    "\n",
    "STOPWORDS = [\"covid\",'america',\"ncov\",\"covid-19\",\"covid19\",\"coronavirus\",\"cov\",\"us\",\"americans\",\"fda\",\"dr\",\"corona\",\"north\",\"south\",\"state\",\"university\",\"merck merck\"]\n",
    "\n",
    "def generate_wordcloud(df, stopwords):\n",
    "    words = [w.replace(\"#\",\"\").replace(\"@\",\"\").lower() for w \n",
    "                                in \"|\".join(df.ner.values.tolist()).split(\"|\")]\n",
    "    words = [w for w in words if len(w)>=2]\n",
    "    all_word_string = \",\".join(words).lower().replace(\"food and\",\"FDA\").replace(\"and drug\",\"FDA\").replace(\"president trump\",\"trump\").replace(\"university of\",\"university\").replace(\"merck merck\",\"merck\")\n",
    "    # stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(random_state=1, \n",
    "                        stopwords=stopwords,\n",
    "                        background_color=\"white\",\n",
    "                        max_words=80, \n",
    "                        contour_width=3, \n",
    "                        min_word_length = 2,\n",
    "                        width=600, height=400,\n",
    "                        # min_font_size = 20,\n",
    "                        max_font_size = 100,\n",
    "                        color_func=grey_color_func).generate(all_word_string)\n",
    "\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "keywords_dict = {\"hcq\":\"Hydroxychloroquine|hcq|plaque|plaquenil|hydroquine|axemal\",\n",
    "                    \"ivermectin\": \"ivermectin|stromectol|soolantra|sklice|ivm\",\n",
    "                    \"remdesivir\": \"remdesivir|veklury|rem\",\n",
    "                    \"molnupiravir\": \"molnupiravir|merck's drug|merck's pill|merck's antiviral|merck's\"}\n",
    "\n",
    "files = sorted(glob.glob(\"../data/ner/*\"))\n",
    "\n",
    "all_wcs = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    # pos, neg = [], [] \n",
    "    drug_wcs = []\n",
    "    drug = file.split(\"/\")[-1]\n",
    "    stopwords = keywords_dict[drug].split(\"|\")\n",
    "    stopwords.extend(STOPWORDS)\n",
    "    df = pd.read_csv(file)\n",
    "    for i in [1,2,3]:\n",
    "        # print(\"On wave %i\"%i)\n",
    "        df = df[~df.ner.isna()]\n",
    "        w = df[df.wave==i]\n",
    "        # print(Counter(words).most_common()[:20])\n",
    "        drug_wcs.append(generate_wordcloud(w,stopwords))\n",
    "    all_wcs.append(drug_wcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-918a5ac73964>:2: FutureWarning: The input object of type 'WordCloud' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'WordCloud', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  all_wcs = np.array(all_wcs).T\n",
      "<ipython-input-5-918a5ac73964>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_wcs = np.array(all_wcs).T\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_wcs = np.array(all_wcs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(3, 4,  figsize=(140, 70), sharex=True, sharey=True)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        axs[i][j].imshow(all_wcs[i][j])\n",
    "        axs[i][j].axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"wordcloud.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0babe654b855458543363a4b830ff43f68cfb343983d247d42ea7e3e37bc8d56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('twee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
