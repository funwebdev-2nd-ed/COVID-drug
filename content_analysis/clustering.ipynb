{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering\n",
    "\n",
    "In this notebook we use BERTweet to encode all tweets, use PCA to reduce their dimensions, and use KMeans to cluster them into 15 topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/final/hcq.csv',\n",
       " '../data/final/ivermectin.csv',\n",
       " '../data/final/molnupiravir.csv',\n",
       " '../data/final/remdesivir.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "data_dir = \"../data/final/\"\n",
    "files = sorted(glob.glob(f\"{data_dir}*.csv\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hcq', 'ivermectin', 'molnupiravir', 'remdesivir']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [pd.read_csv(file) for file in files] \n",
    "drugs = [d.split(\"/\")[-1].split(\".\")[0] for d in files]\n",
    "drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /mnt/hdd/ningh/.cache/torch/sentence_transformers/digitalepidemiologylab_covid-twitter-bert. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /mnt/hdd/ningh/.cache/torch/sentence_transformers/digitalepidemiologylab_covid-twitter-bert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model_name = 'digitalepidemiologylab/covid-twitter-bert'\n",
    "#@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before clustering let's mask the user names and calculate the embeddings. \n",
    "This takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def desensitize(text):\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\"@USER\", text).split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6020/6020 [12:39<00:00,  7.93it/s]\n",
      "Batches: 100%|██████████| 7864/7864 [16:26<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 155/155 [00:17<00:00,  8.72it/s]\n",
      "Batches: 100%|██████████| 1307/1307 [02:39<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for DRUG, df in zip(drugs,dfs):\n",
    "    # df[\"desensitized_text\"] = df.full_text.parallel_apply(lambda x: desensitize(x))\n",
    "    sentences = df.desensitized_text\n",
    "\n",
    "    embeddings = model.encode(sentences,show_progress_bar=True)\n",
    "    embeddings.shape\n",
    "    x = torch.tensor(embeddings)\n",
    "    torch.save(x, f'embeddings/{DRUG}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get backend toolkit code\n",
    "# !wget https://public-thought.media.mit.edu/static/ccc_toolkit_v_21_0.py\n",
    "\n",
    "# # get emotions lexicon\n",
    "# !wget https://saifmohammad.com/WebDocs/Lexicons/NRC-Emotion-Lexicon.zip\n",
    "# !unzip NRC-Emotion-Lexicon.zip\n",
    "\n",
    "# Download and load data (this is going to take a couple of minutes.)\n",
    "from ccc_toolkit_v_21_0 import (\n",
    "                          plot_tsne_viz,\n",
    "                          run_clustering,\n",
    "                          reduce_dimensionality,\n",
    "                          retrieve,\n",
    "                          SentenceTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "We'll load the embeddings and run the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Every time you run this cell you'll get different clusters and embeddings\n",
    "due to the fact that these methods calculate approximate transformations\n",
    "that depend on the initial random seed.\n",
    "\"\"\"\n",
    "\n",
    "N_TOPICS = 15\n",
    "\n",
    "import torch\n",
    "\n",
    "def viz_cluster(df, pca_components=30, n_clusters=N_TOPICS):\n",
    "    sub_df = df\n",
    "    utt = sub_df.full_text.tolist()\n",
    "    emb = torch.from_numpy(np.concatenate(sub_df.embeddings.values).reshape(len(sub_df),1024))\n",
    "    print(\"Sampled data contains {} utterances.\".format(len(emb)))\n",
    "\n",
    "    reduced_embeddings = reduce_dimensionality(emb)\n",
    "    viz_coord, centers, predictions, _, _ = run_clustering(utt,\n",
    "                                                            pca_components=pca_components,\n",
    "                                                            k_clusters=n_clusters,\n",
    "                                                            embeddings=reduced_embeddings)\n",
    "    # plot_tsne_viz(viz_coord, utt,\n",
    "    #                 clusters = predictions, \n",
    "    #                 # centers = sub_df.center.tolist(),\n",
    "    #                 coloring='clusters',\n",
    "    #                 # labels = sub_df.med.tolist(),\n",
    "    #                 title=\"%i-means clusters.\"%n_clusters)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"clusters_negative/\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This divides each drug into 3 waves. But we found that people's rationale barely change across 3 waves, so we removeed it.\n",
    "# for i, drug in enumerate(drugs):\n",
    "#     print(\"On %s\"%drug)\n",
    "#     embeddings = torch.load(f\"embeddings/{drug}.pt\")\n",
    "#     print(embeddings.shape)\n",
    "#     df = dfs[I]\n",
    "#     # df[\"desensitized_text\"] = df.full_text.parallel_apply(lambda x: desensitize(x))\n",
    "\n",
    "#     df[\"embeddings\"] = [np.array(e) for e in embeddings]\n",
    "#     df = df[df.stance==-1]\n",
    "#     df[\"lower\"] = df.desensitized_text.str.lower()\n",
    "#     df = df.drop_duplicates(subset=['lower'])\n",
    "#     df.pop(\"lower\")\n",
    "\n",
    "#     w1, w2, w3 = df[df.wave==1], df[df.wave==2], df[df.wave==3]\n",
    "#     m = 1 \n",
    "#     for w in [w1, w2, w3]:\n",
    "#         if len(w) < N_TOPICS*3:\n",
    "#             print(\"Too few tweets. Skipping...\")\n",
    "#             continue\n",
    "#         w = w.reset_index(drop=True)\n",
    "#         # df_cent = center_df[(center_df.drug==drug)&(center_df.wave==m)]\n",
    "#         emb = torch.from_numpy(np.concatenate(w.embeddings.values).reshape(len(w),1024))\n",
    "#         center_idx = viz_cluster(w)\n",
    "#         df_cent = w.iloc[center_idx][\"desensitized_text\"].reset_index(drop=True)\n",
    "#         ## find similar 15 tweets in the pool of that drug, that time window\n",
    "#         # print(\"Start retrieving\")\n",
    "#         emb = emb.to(device)\n",
    "\n",
    "#         w_similar_df = retrieve(model, df_cent.values, w.desensitized_text, emb, closest_n=30)\n",
    "#         w_similar_df.to_csv(f\"clusters_negative/{drug}_wave{m}_cluster.csv\",index=False)\n",
    "#         m += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, drug in enumerate(drugs):\n",
    "    print(\"On %s\"%drug)\n",
    "    embeddings = torch.load(f\"embeddings/{drug}.pt\")\n",
    "    print(embeddings.shape)\n",
    "    df = dfs[i]\n",
    "    # df[\"desensitized_text\"] = df.full_text.parallel_apply(lambda x: desensitize(x))\n",
    "    df[\"embeddings\"] = [np.array(e) for e in embeddings]\n",
    "    df = df[df.stance==-1]\n",
    "    df[\"lower\"] = df.desensitized_text.str.lower()\n",
    "    df = df.drop_duplicates(subset=['lower'])\n",
    "    df.pop(\"lower\")\n",
    "\n",
    "    w = df.reset_index(drop=True)\n",
    "    emb = torch.from_numpy(np.concatenate(w.embeddings.values).reshape(len(w),1024))\n",
    "    center_idx = viz_cluster(w)\n",
    "    df_cent = w.iloc[center_idx][\"desensitized_text\"].reset_index(drop=True)\n",
    "    ## find similar 15 tweets in the pool of that drug, that time window\n",
    "    # print(\"Start retrieving\")\n",
    "    emb = emb.to(device)\n",
    "\n",
    "    w_similar_df = retrieve(model, df_cent.values, w.desensitized_text, emb, closest_n=30)\n",
    "    w_similar_df.to_csv(f\"clusters_negative/{drug}_cluster.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0babe654b855458543363a4b830ff43f68cfb343983d247d42ea7e3e37bc8d56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('twee': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
